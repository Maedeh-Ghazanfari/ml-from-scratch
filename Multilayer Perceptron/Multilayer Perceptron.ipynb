{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a2b8b98f-febe-46a8-bb61-fb84f1e52383",
   "metadata": {},
   "outputs": [],
   "source": [
    "# I tried my best to avoid hard coding in this project:)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffffbfe3-3a21-4098-bce7-7dcdef3a4ff9",
   "metadata": {},
   "source": [
    "#### Importing libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "28b3f1bb-2cdf-4359-aec2-c060747850f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.misc import derivative"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8301bfab-017d-4a3d-8fed-2190f92c914b",
   "metadata": {},
   "source": [
    "#### Defining the dataset which is simply XOR:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9fa541f1-b6c2-4dd2-971f-7770343b3fb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array([[0,0], [0,1], [1,1], [1,0]]) # inputs\n",
    "y = np.array([[0], [1], [0], [1]]) # labels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5a9ee98-6785-48ff-b347-692701a5e678",
   "metadata": {},
   "source": [
    "#### Initialize the layers size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f6b8df3c-191d-41dd-bcfc-1c50991bb192",
   "metadata": {},
   "outputs": [],
   "source": [
    "network = {'Input':2, 'Hidden_1': 3, 'Hidden_2':2, 'Output': 1}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85b39346-4afc-4cd0-bb20-0417a319a94f",
   "metadata": {},
   "source": [
    "#### Define weights, biases and X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a9a0b553-0b65-4d73-b09d-d271e05fa491",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First I wanted to define weight this simple but then I learned something cooler;)\n",
    "# W = np.random.randn(2,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5c7d4d34-37a0-4b20-a3a8-ff3491fab163",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Can initialize weights using Xavier or He normal and based on my activation functions:\n",
    "def init_weights(n_in, n_out, activation) -> np.ndarray:\n",
    "    if activation in ['sigmoid', 'tanh']:\n",
    "        limit = np.sqrt(6 / (n_in + n_out))  # Xavier uniform\n",
    "        weights = np.random.uniform(-limit, limit, size=(n_in, n_out))\n",
    "    elif activation == 'relu':\n",
    "        std_dev = np.sqrt(2 / n_in)  # He normal\n",
    "        weights = np.random.normal(0, std_dev, size=(n_in, n_out))\n",
    "    else:\n",
    "        weights = np.random.normal(0, 0.01, size=(n_in, n_out))\n",
    "    return weights\n",
    "\n",
    "# Initialize biases based on count of neurons of each layer\n",
    "def init_bias(n_neurons):\n",
    "    biases = np.zeros((1, n_neurons))\n",
    "    return biases"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "349c2f32-b1a5-4542-9a13-c455c903566b",
   "metadata": {},
   "source": [
    "#### Activation Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5485965e-5e7d-4b2c-b928-bfae2d1680de",
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu_func(x:np.ndarray) -> np.ndarray:\n",
    "    return(np.maximum(0,x))\n",
    "\n",
    "def relu_derivative(a):\n",
    "    return (a > 0).astype(float)\n",
    "\n",
    "def sigmoid_func(x:np.ndarray) -> np.ndarray:\n",
    "    # sigmoid returns probabilities and not labels!\n",
    "    p = 1 / (1+np.exp(-x))\n",
    "    \n",
    "    labels = (p >= 0.5).astype(int) # this works as boolean, if p>=0.5 returns True and then astype(int) converts to 1!\n",
    "    return labels\n",
    "\n",
    "def sigmoid_derivative(a):\n",
    "    return a * (1 - a)\n",
    "\n",
    "activations = {'relu': relu_func, 'relu_der': relu_derivative, 'sigmoid': sigmoid_func, 'sigmoid_der': sigmoid_derivative}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0debc97-91bf-4013-9e31-27e8db9c4e55",
   "metadata": {},
   "source": [
    "#### FeedForward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9cd894ea-edb8-4991-963d-2d9cea394a16",
   "metadata": {},
   "outputs": [],
   "source": [
    "def feedforward(W:np.ndarray, b:np.ndarray, X:np.ndarray, activation_func:callable):\n",
    "    z = (np.dot(X,W)) + b\n",
    "    a = activation_func(z) # adding activation function \n",
    "    return a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "dae67a9f-169e-4e79-89c9-1503a88d8a4e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "a = X\n",
    "weights = {} # storing different layers weights inside a dict.\n",
    "biases = {}  # same for biases.\n",
    "after_act = {}\n",
    "\n",
    "# A loop  for initializing parameters and feedforward\n",
    "for index, (key, value) in enumerate(network.items()):\n",
    "    if key == 'Input':\n",
    "        pass\n",
    "    elif 'Hidden' in key:\n",
    "        w = init_weights(list(network.values())[index-1], list(network.values())[index],activations['relu'])\n",
    "        b = init_bias(value)\n",
    "        \n",
    "        assert w.shape[0] == a.shape[1] # Preventing error because of a shape mismatch between matrices\n",
    "        a = feedforward(w,b,a, activations[\"relu\"])\n",
    "        \n",
    "        weights[f\"Hidden_{index}\"] = w\n",
    "        biases[f\"Hidden_{index}\"] = b\n",
    "        after_act[f\"Hidden_{index}\"] = a\n",
    "        \n",
    "    else:\n",
    "        w = init_weights(list(network.values())[index-1], list(network.values())[index],activations['sigmoid']) # dicts don't have index function, converting to list first\n",
    "        b = init_bias(value)\n",
    "        \n",
    "        assert w.shape[0] == a.shape[1] # Preventing error because of a shape mismatch between matrices\n",
    "        a = feedforward(w,b,a, activations[\"sigmoid\"])\n",
    "        \n",
    "        weights[f\"Output\"] = w\n",
    "        biases[f\"Output\"] = b\n",
    "        after_act[f\"Output\"] = a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "40cc1280-c975-4f1f-a181-5b36faa7e078",
   "metadata": {},
   "outputs": [],
   "source": [
    "epsilon = 1e-15 # log(0) is undefined\n",
    "y_hat = np.clip(after_act['Output'], epsilon, 1 - epsilon)\n",
    "loss = -np.mean(y * np.log(y_hat) + (1 - y) * np.log(1 - y_hat))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4ce65bd-24bd-46ca-b271-fe9e310af75f",
   "metadata": {},
   "source": [
    "#### BackPropagation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8635b4fd-cdaa-4f53-bea9-af89f77278c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculating the error and delta\n",
    "\n",
    "deltas = {}\n",
    "\n",
    "# Output layer delta\n",
    "error = y - after_act['Output']\n",
    "deltas['Output'] = error * sigmoid_derivative(after_act['Output'])\n",
    "\n",
    "# Hidden layers in reverse order\n",
    "hidden_layers = [k for k in weights.keys() if 'Hidden' in k]\n",
    "hidden_layers.sort(reverse=True)  # or reverse order of insertion if needed\n",
    "\n",
    "next_layer = 'Output'\n",
    "for layer in hidden_layers:\n",
    "    deltas[layer] = np.dot(deltas[next_layer], weights[next_layer].T) * relu_derivative(after_act[layer])\n",
    "    next_layer = layer\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "2d17e1dc-7495-4128-b3a9-a6b1d0763a16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calcuating gradient\n",
    "# bias\n",
    "grad_b = np.sum(deltas, axis=0, keepdims=True)\n",
    "\n",
    "# weight\n",
    "grad_w = {}\n",
    "# I know I coded this part really weird-_-\n",
    "for index_d, (key_d, value_d) in enumerate(deltas.items()):\n",
    "    for index_a, (key_a, value_a) in enumerate(after_act.items()):\n",
    "        if key_d == key_a:\n",
    "            if key_d == 'Hidden_1':\n",
    "                grad_w[key_d] = np.dot(X.T, value_d)\n",
    "            else:\n",
    "                grad_w[key_d] = np.dot(np.transpose(list(after_act.values())[index_a-1]), value_d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6e84f20-aedb-4709-a4f5-3e3391d01ed0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
