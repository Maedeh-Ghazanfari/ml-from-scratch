{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a2b8b98f-febe-46a8-bb61-fb84f1e52383",
   "metadata": {},
   "outputs": [],
   "source": [
    "# I tried my best to avoid hard coding in this project:)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffffbfe3-3a21-4098-bce7-7dcdef3a4ff9",
   "metadata": {},
   "source": [
    "#### Importing libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "28b3f1bb-2cdf-4359-aec2-c060747850f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sympy import symbols, diff\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8301bfab-017d-4a3d-8fed-2190f92c914b",
   "metadata": {},
   "source": [
    "#### Defining the dataset which is simply XOR:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9fa541f1-b6c2-4dd2-971f-7770343b3fb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array([[0,0], [0,1], [1,1], [1,0]]) # inputs\n",
    "y = np.array([[0], [1], [0], [1]]) # labels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5a9ee98-6785-48ff-b347-692701a5e678",
   "metadata": {},
   "source": [
    "#### Initialize the layers size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f6b8df3c-191d-41dd-bcfc-1c50991bb192",
   "metadata": {},
   "outputs": [],
   "source": [
    "network = {'Input':2, 'Hidden_1': 3, 'Hidden_2':2, 'Output': 1}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85b39346-4afc-4cd0-bb20-0417a319a94f",
   "metadata": {},
   "source": [
    "#### Define weights, biases and X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5c7d4d34-37a0-4b20-a3a8-ff3491fab163",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Can initialize weights using Xavier or He normal and based on my activation functions:\n",
    "def init_weights(n_in, n_out, activation) -> np.ndarray:\n",
    "    if activation in ['sigmoid', 'tanh']:\n",
    "        limit = np.sqrt(6 / (n_in + n_out))  # Xavier uniform\n",
    "        weights = np.random.uniform(-limit, limit, size=(n_in, n_out))\n",
    "    elif activation == 'relu':\n",
    "        std_dev = np.sqrt(2 / n_in)  # He normal\n",
    "        weights = np.random.normal(0, std_dev, size=(n_in, n_out))\n",
    "    else:\n",
    "        weights = np.random.normal(0, 0.01, size=(n_in, n_out))\n",
    "    return weights\n",
    "\n",
    "# Initialize biases based on count of neurons of each layer\n",
    "def init_bias(n_neurons):\n",
    "    biases = np.zeros((1, n_neurons))\n",
    "    return biases"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "349c2f32-b1a5-4542-9a13-c455c903566b",
   "metadata": {},
   "source": [
    "#### Activation Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5485965e-5e7d-4b2c-b928-bfae2d1680de",
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu_func(x:np.ndarray) -> np.ndarray:\n",
    "    return(np.maximum(0,x))\n",
    "\n",
    "def relu_derivative(a):\n",
    "    return (a > 0).astype(float)\n",
    "\n",
    "def sigmoid_func(x:np.ndarray) -> np.ndarray:\n",
    "    # sigmoid returns probabilities and not labels!\n",
    "    p = 1 / (1+np.exp(-x))\n",
    "    \n",
    "    labels = (p >= 0.5).astype(int) # this works as boolean, if p>=0.5 returns True and then astype(int) converts to 1!\n",
    "    return labels\n",
    "\n",
    "def sigmoid_derivative(a):\n",
    "    return a * (1 - a)\n",
    "\n",
    "activations = {'relu': relu_func, 'relu_der': relu_derivative, 'sigmoid': sigmoid_func, 'sigmoid_der': sigmoid_derivative}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8ce5717a-b0f8-409b-a69f-cfa02c715cbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "lr = 0.01        # learning rate\n",
    "beta1 = 0.9      # momentum term\n",
    "beta2 = 0.999    # RMSprop term\n",
    "epsilon = 1e-8   # small value to prevent division by zero\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7ab49078-043f-4b4b-8981-0691579e7f12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initializing weights and biases\n",
    "weights = {} # storing different layers weights inside a dict.\n",
    "biases = {}  # same for biases.\n",
    "\n",
    "for index, (key, value) in enumerate(network.items()):\n",
    "    if key == 'Input':\n",
    "        continue\n",
    "    elif 'Hidden' in key:\n",
    "        w = init_weights(list(network.values())[index-1], value, activations['relu'])\n",
    "        b = np.zeros((1, value))\n",
    "        weights[f'Hidden_{index}'] = w\n",
    "        biases[f'Hidden_{index}'] = b\n",
    "    else:\n",
    "        w = init_weights(list(network.values())[index-1], value, activations['sigmoid'])\n",
    "        b = init_bias(value)\n",
    "        weights['Output'] = w\n",
    "        biases['Output'] = b\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "30ccb02d-c10f-47fb-b1f1-03b67f02d6f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adam variables\n",
    "m_w = {k: np.zeros_like(v) for k,v in weights.items()}\n",
    "v_w = {k: np.zeros_like(v) for k,v in weights.items()}\n",
    "m_b = {k: np.zeros_like(v) for k,v in biases.items()}\n",
    "v_b = {k: np.zeros_like(v) for k,v in biases.items()}\n",
    "t = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "afca1102-e01a-4833-8030-d2110e649316",
   "metadata": {},
   "outputs": [],
   "source": [
    "def feedforward(W, b, X, activation_func):\n",
    "    z = np.dot(X, W) + b\n",
    "    return activation_func(z)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53c20747-37cf-4399-8ce3-7a056acbe124",
   "metadata": {},
   "source": [
    "#### Training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6e3fa195-c99b-4edb-9671-7db0b58e8c90",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5, Loss: 9.2103\n",
      "Epoch 10, Loss: 9.2103\n",
      "Epoch 15, Loss: 9.2103\n",
      "Epoch 20, Loss: 9.2103\n",
      "Epoch 25, Loss: 9.2103\n",
      "Epoch 30, Loss: 9.2103\n",
      "Epoch 35, Loss: 9.2103\n",
      "Epoch 40, Loss: 9.2103\n",
      "Epoch 45, Loss: 9.2103\n",
      "Epoch 50, Loss: 9.2103\n",
      "Epoch 55, Loss: 9.2103\n",
      "Epoch 60, Loss: 9.2103\n",
      "Epoch 65, Loss: 9.2103\n",
      "Epoch 70, Loss: 9.2103\n",
      "Epoch 75, Loss: 9.2103\n",
      "Epoch 80, Loss: 9.2103\n",
      "Epoch 85, Loss: 9.2103\n",
      "Epoch 90, Loss: 9.2103\n",
      "Epoch 95, Loss: 9.2103\n",
      "Epoch 100, Loss: 9.2103\n",
      "Epoch 105, Loss: 9.2103\n",
      "Epoch 110, Loss: 9.2103\n",
      "Epoch 115, Loss: 9.2103\n",
      "Epoch 120, Loss: 9.2103\n",
      "Epoch 125, Loss: 9.2103\n",
      "Epoch 130, Loss: 9.2103\n",
      "Epoch 135, Loss: 9.2103\n",
      "Epoch 140, Loss: 9.2103\n",
      "Epoch 145, Loss: 9.2103\n",
      "Epoch 150, Loss: 9.2103\n",
      "Epoch 155, Loss: 9.2103\n",
      "Epoch 160, Loss: 9.2103\n",
      "Epoch 165, Loss: 9.2103\n",
      "Epoch 170, Loss: 9.2103\n",
      "Epoch 175, Loss: 9.2103\n",
      "Epoch 180, Loss: 9.2103\n",
      "Epoch 185, Loss: 9.2103\n",
      "Epoch 190, Loss: 9.2103\n",
      "Epoch 195, Loss: 9.2103\n",
      "Epoch 200, Loss: 9.2103\n",
      "Epoch 205, Loss: 9.2103\n",
      "Epoch 210, Loss: 9.2103\n",
      "Epoch 215, Loss: 9.2103\n",
      "Epoch 220, Loss: 9.2103\n",
      "Epoch 225, Loss: 9.2103\n",
      "Epoch 230, Loss: 9.2103\n",
      "Epoch 235, Loss: 9.2103\n",
      "Epoch 240, Loss: 9.2103\n",
      "Epoch 245, Loss: 9.2103\n",
      "Epoch 250, Loss: 9.2103\n",
      "Epoch 255, Loss: 9.2103\n",
      "Epoch 260, Loss: 9.2103\n",
      "Epoch 265, Loss: 9.2103\n",
      "Epoch 270, Loss: 9.2103\n",
      "Epoch 275, Loss: 9.2103\n",
      "Epoch 280, Loss: 9.2103\n",
      "Epoch 285, Loss: 9.2103\n",
      "Epoch 290, Loss: 9.2103\n",
      "Epoch 295, Loss: 9.2103\n",
      "Epoch 300, Loss: 9.2103\n",
      "Epoch 305, Loss: 9.2103\n",
      "Epoch 310, Loss: 9.2103\n",
      "Epoch 315, Loss: 9.2103\n",
      "Epoch 320, Loss: 9.2103\n",
      "Epoch 325, Loss: 9.2103\n",
      "Epoch 330, Loss: 9.2103\n",
      "Epoch 335, Loss: 9.2103\n",
      "Epoch 340, Loss: 9.2103\n",
      "Epoch 345, Loss: 9.2103\n",
      "Epoch 350, Loss: 9.2103\n",
      "Epoch 355, Loss: 9.2103\n",
      "Epoch 360, Loss: 9.2103\n",
      "Epoch 365, Loss: 9.2103\n",
      "Epoch 370, Loss: 9.2103\n",
      "Epoch 375, Loss: 9.2103\n",
      "Epoch 380, Loss: 9.2103\n",
      "Epoch 385, Loss: 9.2103\n",
      "Epoch 390, Loss: 9.2103\n",
      "Epoch 395, Loss: 9.2103\n",
      "Epoch 400, Loss: 9.2103\n",
      "Epoch 405, Loss: 9.2103\n",
      "Epoch 410, Loss: 9.2103\n",
      "Epoch 415, Loss: 9.2103\n",
      "Epoch 420, Loss: 9.2103\n",
      "Epoch 425, Loss: 9.2103\n",
      "Epoch 430, Loss: 9.2103\n",
      "Epoch 435, Loss: 9.2103\n",
      "Epoch 440, Loss: 9.2103\n",
      "Epoch 445, Loss: 9.2103\n",
      "Epoch 450, Loss: 9.2103\n",
      "Epoch 455, Loss: 9.2103\n",
      "Epoch 460, Loss: 9.2103\n",
      "Epoch 465, Loss: 9.2103\n",
      "Epoch 470, Loss: 9.2103\n",
      "Epoch 475, Loss: 9.2103\n",
      "Epoch 480, Loss: 9.2103\n",
      "Epoch 485, Loss: 9.2103\n",
      "Epoch 490, Loss: 9.2103\n",
      "Epoch 495, Loss: 9.2103\n",
      "Epoch 500, Loss: 9.2103\n",
      "Epoch 505, Loss: 9.2103\n",
      "Epoch 510, Loss: 9.2103\n",
      "Epoch 515, Loss: 9.2103\n",
      "Epoch 520, Loss: 9.2103\n",
      "Epoch 525, Loss: 9.2103\n",
      "Epoch 530, Loss: 9.2103\n",
      "Epoch 535, Loss: 9.2103\n",
      "Epoch 540, Loss: 9.2103\n",
      "Epoch 545, Loss: 9.2103\n",
      "Epoch 550, Loss: 9.2103\n",
      "Epoch 555, Loss: 9.2103\n",
      "Epoch 560, Loss: 9.2103\n",
      "Epoch 565, Loss: 9.2103\n",
      "Epoch 570, Loss: 9.2103\n",
      "Epoch 575, Loss: 9.2103\n",
      "Epoch 580, Loss: 9.2103\n",
      "Epoch 585, Loss: 9.2103\n",
      "Epoch 590, Loss: 9.2103\n",
      "Epoch 595, Loss: 9.2103\n",
      "Epoch 600, Loss: 9.2103\n",
      "Epoch 605, Loss: 9.2103\n",
      "Epoch 610, Loss: 9.2103\n",
      "Epoch 615, Loss: 9.2103\n",
      "Epoch 620, Loss: 9.2103\n",
      "Epoch 625, Loss: 9.2103\n",
      "Epoch 630, Loss: 9.2103\n",
      "Epoch 635, Loss: 9.2103\n",
      "Epoch 640, Loss: 9.2103\n",
      "Epoch 645, Loss: 9.2103\n",
      "Epoch 650, Loss: 9.2103\n",
      "Epoch 655, Loss: 9.2103\n",
      "Epoch 660, Loss: 9.2103\n",
      "Epoch 665, Loss: 9.2103\n",
      "Epoch 670, Loss: 9.2103\n",
      "Epoch 675, Loss: 9.2103\n",
      "Epoch 680, Loss: 9.2103\n",
      "Epoch 685, Loss: 9.2103\n",
      "Epoch 690, Loss: 9.2103\n",
      "Epoch 695, Loss: 9.2103\n",
      "Epoch 700, Loss: 9.2103\n",
      "Epoch 705, Loss: 9.2103\n",
      "Epoch 710, Loss: 9.2103\n",
      "Epoch 715, Loss: 9.2103\n",
      "Epoch 720, Loss: 9.2103\n",
      "Epoch 725, Loss: 9.2103\n",
      "Epoch 730, Loss: 9.2103\n",
      "Epoch 735, Loss: 9.2103\n",
      "Epoch 740, Loss: 9.2103\n",
      "Epoch 745, Loss: 9.2103\n",
      "Epoch 750, Loss: 9.2103\n",
      "Epoch 755, Loss: 9.2103\n",
      "Epoch 760, Loss: 9.2103\n",
      "Epoch 765, Loss: 9.2103\n",
      "Epoch 770, Loss: 9.2103\n",
      "Epoch 775, Loss: 9.2103\n",
      "Epoch 780, Loss: 9.2103\n",
      "Epoch 785, Loss: 9.2103\n",
      "Epoch 790, Loss: 9.2103\n",
      "Epoch 795, Loss: 9.2103\n",
      "Epoch 800, Loss: 9.2103\n",
      "Epoch 805, Loss: 9.2103\n",
      "Epoch 810, Loss: 9.2103\n",
      "Epoch 815, Loss: 9.2103\n",
      "Epoch 820, Loss: 9.2103\n",
      "Epoch 825, Loss: 9.2103\n",
      "Epoch 830, Loss: 9.2103\n",
      "Epoch 835, Loss: 9.2103\n",
      "Epoch 840, Loss: 9.2103\n",
      "Epoch 845, Loss: 9.2103\n",
      "Epoch 850, Loss: 9.2103\n",
      "Epoch 855, Loss: 9.2103\n",
      "Epoch 860, Loss: 9.2103\n",
      "Epoch 865, Loss: 9.2103\n",
      "Epoch 870, Loss: 9.2103\n",
      "Epoch 875, Loss: 9.2103\n",
      "Epoch 880, Loss: 9.2103\n",
      "Epoch 885, Loss: 9.2103\n",
      "Epoch 890, Loss: 9.2103\n",
      "Epoch 895, Loss: 9.2103\n",
      "Epoch 900, Loss: 9.2103\n",
      "Epoch 905, Loss: 9.2103\n",
      "Epoch 910, Loss: 9.2103\n",
      "Epoch 915, Loss: 9.2103\n",
      "Epoch 920, Loss: 9.2103\n",
      "Epoch 925, Loss: 9.2103\n",
      "Epoch 930, Loss: 9.2103\n",
      "Epoch 935, Loss: 9.2103\n",
      "Epoch 940, Loss: 9.2103\n",
      "Epoch 945, Loss: 9.2103\n",
      "Epoch 950, Loss: 9.2103\n",
      "Epoch 955, Loss: 9.2103\n",
      "Epoch 960, Loss: 9.2103\n",
      "Epoch 965, Loss: 9.2103\n",
      "Epoch 970, Loss: 9.2103\n",
      "Epoch 975, Loss: 9.2103\n",
      "Epoch 980, Loss: 9.2103\n",
      "Epoch 985, Loss: 9.2103\n",
      "Epoch 990, Loss: 9.2103\n",
      "Epoch 995, Loss: 9.2103\n",
      "Epoch 1000, Loss: 9.2103\n",
      "Epoch 1005, Loss: 9.2103\n",
      "Epoch 1010, Loss: 9.2103\n",
      "Epoch 1015, Loss: 9.2103\n",
      "Epoch 1020, Loss: 9.2103\n",
      "Epoch 1025, Loss: 9.2103\n",
      "Epoch 1030, Loss: 9.2103\n",
      "Epoch 1035, Loss: 9.2103\n",
      "Epoch 1040, Loss: 9.2103\n",
      "Epoch 1045, Loss: 9.2103\n",
      "Epoch 1050, Loss: 9.2103\n",
      "Epoch 1055, Loss: 9.2103\n",
      "Epoch 1060, Loss: 9.2103\n",
      "Epoch 1065, Loss: 9.2103\n",
      "Epoch 1070, Loss: 9.2103\n",
      "Epoch 1075, Loss: 9.2103\n",
      "Epoch 1080, Loss: 9.2103\n",
      "Epoch 1085, Loss: 9.2103\n",
      "Epoch 1090, Loss: 9.2103\n",
      "Epoch 1095, Loss: 9.2103\n",
      "Epoch 1100, Loss: 9.2103\n",
      "Epoch 1105, Loss: 9.2103\n",
      "Epoch 1110, Loss: 9.2103\n",
      "Epoch 1115, Loss: 9.2103\n",
      "Epoch 1120, Loss: 9.2103\n",
      "Epoch 1125, Loss: 9.2103\n",
      "Epoch 1130, Loss: 9.2103\n",
      "Epoch 1135, Loss: 9.2103\n",
      "Epoch 1140, Loss: 9.2103\n",
      "Epoch 1145, Loss: 9.2103\n",
      "Epoch 1150, Loss: 9.2103\n",
      "Epoch 1155, Loss: 9.2103\n",
      "Epoch 1160, Loss: 9.2103\n",
      "Epoch 1165, Loss: 9.2103\n",
      "Epoch 1170, Loss: 9.2103\n",
      "Epoch 1175, Loss: 9.2103\n",
      "Epoch 1180, Loss: 9.2103\n",
      "Epoch 1185, Loss: 9.2103\n",
      "Epoch 1190, Loss: 9.2103\n",
      "Epoch 1195, Loss: 9.2103\n",
      "Epoch 1200, Loss: 9.2103\n",
      "Epoch 1205, Loss: 9.2103\n",
      "Epoch 1210, Loss: 9.2103\n",
      "Epoch 1215, Loss: 9.2103\n",
      "Epoch 1220, Loss: 9.2103\n",
      "Epoch 1225, Loss: 9.2103\n",
      "Epoch 1230, Loss: 9.2103\n",
      "Epoch 1235, Loss: 9.2103\n",
      "Epoch 1240, Loss: 9.2103\n",
      "Epoch 1245, Loss: 9.2103\n",
      "Epoch 1250, Loss: 9.2103\n",
      "Epoch 1255, Loss: 9.2103\n",
      "Epoch 1260, Loss: 9.2103\n",
      "Epoch 1265, Loss: 9.2103\n",
      "Epoch 1270, Loss: 9.2103\n",
      "Epoch 1275, Loss: 9.2103\n",
      "Epoch 1280, Loss: 9.2103\n",
      "Epoch 1285, Loss: 9.2103\n",
      "Epoch 1290, Loss: 9.2103\n",
      "Epoch 1295, Loss: 9.2103\n",
      "Epoch 1300, Loss: 9.2103\n",
      "Epoch 1305, Loss: 9.2103\n",
      "Epoch 1310, Loss: 9.2103\n",
      "Epoch 1315, Loss: 9.2103\n",
      "Epoch 1320, Loss: 9.2103\n",
      "Epoch 1325, Loss: 9.2103\n",
      "Epoch 1330, Loss: 9.2103\n",
      "Epoch 1335, Loss: 9.2103\n",
      "Epoch 1340, Loss: 9.2103\n",
      "Epoch 1345, Loss: 9.2103\n",
      "Epoch 1350, Loss: 9.2103\n",
      "Epoch 1355, Loss: 9.2103\n",
      "Epoch 1360, Loss: 9.2103\n",
      "Epoch 1365, Loss: 9.2103\n",
      "Epoch 1370, Loss: 9.2103\n",
      "Epoch 1375, Loss: 9.2103\n",
      "Epoch 1380, Loss: 9.2103\n",
      "Epoch 1385, Loss: 9.2103\n",
      "Epoch 1390, Loss: 9.2103\n",
      "Epoch 1395, Loss: 9.2103\n",
      "Epoch 1400, Loss: 9.2103\n",
      "Epoch 1405, Loss: 9.2103\n",
      "Epoch 1410, Loss: 9.2103\n",
      "Epoch 1415, Loss: 9.2103\n",
      "Epoch 1420, Loss: 9.2103\n",
      "Epoch 1425, Loss: 9.2103\n",
      "Epoch 1430, Loss: 9.2103\n",
      "Epoch 1435, Loss: 9.2103\n",
      "Epoch 1440, Loss: 9.2103\n",
      "Epoch 1445, Loss: 9.2103\n",
      "Epoch 1450, Loss: 9.2103\n",
      "Epoch 1455, Loss: 9.2103\n",
      "Epoch 1460, Loss: 9.2103\n",
      "Epoch 1465, Loss: 9.2103\n",
      "Epoch 1470, Loss: 9.2103\n",
      "Epoch 1475, Loss: 9.2103\n",
      "Epoch 1480, Loss: 9.2103\n",
      "Epoch 1485, Loss: 9.2103\n",
      "Epoch 1490, Loss: 9.2103\n",
      "Epoch 1495, Loss: 9.2103\n",
      "Epoch 1500, Loss: 9.2103\n",
      "Epoch 1505, Loss: 9.2103\n",
      "Epoch 1510, Loss: 9.2103\n",
      "Epoch 1515, Loss: 9.2103\n",
      "Epoch 1520, Loss: 9.2103\n",
      "Epoch 1525, Loss: 9.2103\n",
      "Epoch 1530, Loss: 9.2103\n",
      "Epoch 1535, Loss: 9.2103\n",
      "Epoch 1540, Loss: 9.2103\n",
      "Epoch 1545, Loss: 9.2103\n",
      "Epoch 1550, Loss: 9.2103\n",
      "Epoch 1555, Loss: 9.2103\n",
      "Epoch 1560, Loss: 9.2103\n",
      "Epoch 1565, Loss: 9.2103\n",
      "Epoch 1570, Loss: 9.2103\n",
      "Epoch 1575, Loss: 9.2103\n",
      "Epoch 1580, Loss: 9.2103\n",
      "Epoch 1585, Loss: 9.2103\n",
      "Epoch 1590, Loss: 9.2103\n",
      "Epoch 1595, Loss: 9.2103\n",
      "Epoch 1600, Loss: 9.2103\n",
      "Epoch 1605, Loss: 9.2103\n",
      "Epoch 1610, Loss: 9.2103\n",
      "Epoch 1615, Loss: 9.2103\n",
      "Epoch 1620, Loss: 9.2103\n",
      "Epoch 1625, Loss: 9.2103\n",
      "Epoch 1630, Loss: 9.2103\n",
      "Epoch 1635, Loss: 9.2103\n",
      "Epoch 1640, Loss: 9.2103\n",
      "Epoch 1645, Loss: 9.2103\n",
      "Epoch 1650, Loss: 9.2103\n",
      "Epoch 1655, Loss: 9.2103\n",
      "Epoch 1660, Loss: 9.2103\n",
      "Epoch 1665, Loss: 9.2103\n",
      "Epoch 1670, Loss: 9.2103\n",
      "Epoch 1675, Loss: 9.2103\n",
      "Epoch 1680, Loss: 9.2103\n",
      "Epoch 1685, Loss: 9.2103\n",
      "Epoch 1690, Loss: 9.2103\n",
      "Epoch 1695, Loss: 9.2103\n",
      "Epoch 1700, Loss: 9.2103\n",
      "Epoch 1705, Loss: 9.2103\n",
      "Epoch 1710, Loss: 9.2103\n",
      "Epoch 1715, Loss: 9.2103\n",
      "Epoch 1720, Loss: 9.2103\n",
      "Epoch 1725, Loss: 9.2103\n",
      "Epoch 1730, Loss: 9.2103\n",
      "Epoch 1735, Loss: 9.2103\n",
      "Epoch 1740, Loss: 9.2103\n",
      "Epoch 1745, Loss: 9.2103\n",
      "Epoch 1750, Loss: 9.2103\n",
      "Epoch 1755, Loss: 9.2103\n",
      "Epoch 1760, Loss: 9.2103\n",
      "Epoch 1765, Loss: 9.2103\n",
      "Epoch 1770, Loss: 9.2103\n",
      "Epoch 1775, Loss: 9.2103\n",
      "Epoch 1780, Loss: 9.2103\n",
      "Epoch 1785, Loss: 9.2103\n",
      "Epoch 1790, Loss: 9.2103\n",
      "Epoch 1795, Loss: 9.2103\n",
      "Epoch 1800, Loss: 9.2103\n",
      "Epoch 1805, Loss: 9.2103\n",
      "Epoch 1810, Loss: 9.2103\n",
      "Epoch 1815, Loss: 9.2103\n",
      "Epoch 1820, Loss: 9.2103\n",
      "Epoch 1825, Loss: 9.2103\n",
      "Epoch 1830, Loss: 9.2103\n",
      "Epoch 1835, Loss: 9.2103\n",
      "Epoch 1840, Loss: 9.2103\n",
      "Epoch 1845, Loss: 9.2103\n",
      "Epoch 1850, Loss: 9.2103\n",
      "Epoch 1855, Loss: 9.2103\n",
      "Epoch 1860, Loss: 9.2103\n",
      "Epoch 1865, Loss: 9.2103\n",
      "Epoch 1870, Loss: 9.2103\n",
      "Epoch 1875, Loss: 9.2103\n",
      "Epoch 1880, Loss: 9.2103\n",
      "Epoch 1885, Loss: 9.2103\n",
      "Epoch 1890, Loss: 9.2103\n",
      "Epoch 1895, Loss: 9.2103\n",
      "Epoch 1900, Loss: 9.2103\n",
      "Epoch 1905, Loss: 9.2103\n",
      "Epoch 1910, Loss: 9.2103\n",
      "Epoch 1915, Loss: 9.2103\n",
      "Epoch 1920, Loss: 9.2103\n",
      "Epoch 1925, Loss: 9.2103\n",
      "Epoch 1930, Loss: 9.2103\n",
      "Epoch 1935, Loss: 9.2103\n",
      "Epoch 1940, Loss: 9.2103\n",
      "Epoch 1945, Loss: 9.2103\n",
      "Epoch 1950, Loss: 9.2103\n",
      "Epoch 1955, Loss: 9.2103\n",
      "Epoch 1960, Loss: 9.2103\n",
      "Epoch 1965, Loss: 9.2103\n",
      "Epoch 1970, Loss: 9.2103\n",
      "Epoch 1975, Loss: 9.2103\n",
      "Epoch 1980, Loss: 9.2103\n",
      "Epoch 1985, Loss: 9.2103\n",
      "Epoch 1990, Loss: 9.2103\n",
      "Epoch 1995, Loss: 9.2103\n",
      "Epoch 2000, Loss: 9.2103\n",
      "Epoch 2005, Loss: 9.2103\n",
      "Epoch 2010, Loss: 9.2103\n",
      "Epoch 2015, Loss: 9.2103\n",
      "Epoch 2020, Loss: 9.2103\n",
      "Epoch 2025, Loss: 9.2103\n",
      "Epoch 2030, Loss: 9.2103\n",
      "Epoch 2035, Loss: 9.2103\n",
      "Epoch 2040, Loss: 9.2103\n",
      "Epoch 2045, Loss: 9.2103\n",
      "Epoch 2050, Loss: 9.2103\n",
      "Epoch 2055, Loss: 9.2103\n",
      "Epoch 2060, Loss: 9.2103\n",
      "Epoch 2065, Loss: 9.2103\n",
      "Epoch 2070, Loss: 9.2103\n",
      "Epoch 2075, Loss: 9.2103\n",
      "Epoch 2080, Loss: 9.2103\n",
      "Epoch 2085, Loss: 9.2103\n",
      "Epoch 2090, Loss: 9.2103\n",
      "Epoch 2095, Loss: 9.2103\n",
      "Epoch 2100, Loss: 9.2103\n",
      "Epoch 2105, Loss: 9.2103\n",
      "Epoch 2110, Loss: 9.2103\n",
      "Epoch 2115, Loss: 9.2103\n",
      "Epoch 2120, Loss: 9.2103\n",
      "Epoch 2125, Loss: 9.2103\n",
      "Epoch 2130, Loss: 9.2103\n",
      "Epoch 2135, Loss: 9.2103\n",
      "Epoch 2140, Loss: 9.2103\n",
      "Epoch 2145, Loss: 9.2103\n",
      "Epoch 2150, Loss: 9.2103\n",
      "Epoch 2155, Loss: 9.2103\n",
      "Epoch 2160, Loss: 9.2103\n",
      "Epoch 2165, Loss: 9.2103\n",
      "Epoch 2170, Loss: 9.2103\n",
      "Epoch 2175, Loss: 9.2103\n",
      "Epoch 2180, Loss: 9.2103\n",
      "Epoch 2185, Loss: 9.2103\n",
      "Epoch 2190, Loss: 9.2103\n",
      "Epoch 2195, Loss: 9.2103\n",
      "Epoch 2200, Loss: 9.2103\n",
      "Epoch 2205, Loss: 9.2103\n",
      "Epoch 2210, Loss: 9.2103\n",
      "Epoch 2215, Loss: 9.2103\n",
      "Epoch 2220, Loss: 9.2103\n",
      "Epoch 2225, Loss: 9.2103\n",
      "Epoch 2230, Loss: 9.2103\n",
      "Epoch 2235, Loss: 9.2103\n",
      "Epoch 2240, Loss: 9.2103\n",
      "Epoch 2245, Loss: 9.2103\n",
      "Epoch 2250, Loss: 9.2103\n",
      "Epoch 2255, Loss: 9.2103\n",
      "Epoch 2260, Loss: 9.2103\n",
      "Epoch 2265, Loss: 9.2103\n",
      "Epoch 2270, Loss: 9.2103\n",
      "Epoch 2275, Loss: 9.2103\n",
      "Epoch 2280, Loss: 9.2103\n",
      "Epoch 2285, Loss: 9.2103\n",
      "Epoch 2290, Loss: 9.2103\n",
      "Epoch 2295, Loss: 9.2103\n",
      "Epoch 2300, Loss: 9.2103\n",
      "Epoch 2305, Loss: 9.2103\n",
      "Epoch 2310, Loss: 9.2103\n",
      "Epoch 2315, Loss: 9.2103\n",
      "Epoch 2320, Loss: 9.2103\n",
      "Epoch 2325, Loss: 9.2103\n",
      "Epoch 2330, Loss: 9.2103\n",
      "Epoch 2335, Loss: 9.2103\n",
      "Epoch 2340, Loss: 9.2103\n",
      "Epoch 2345, Loss: 9.2103\n",
      "Epoch 2350, Loss: 9.2103\n",
      "Epoch 2355, Loss: 9.2103\n",
      "Epoch 2360, Loss: 9.2103\n",
      "Epoch 2365, Loss: 9.2103\n",
      "Epoch 2370, Loss: 9.2103\n",
      "Epoch 2375, Loss: 9.2103\n",
      "Epoch 2380, Loss: 9.2103\n",
      "Epoch 2385, Loss: 9.2103\n",
      "Epoch 2390, Loss: 9.2103\n",
      "Epoch 2395, Loss: 9.2103\n",
      "Epoch 2400, Loss: 9.2103\n",
      "Epoch 2405, Loss: 9.2103\n",
      "Epoch 2410, Loss: 9.2103\n",
      "Epoch 2415, Loss: 9.2103\n",
      "Epoch 2420, Loss: 9.2103\n",
      "Epoch 2425, Loss: 9.2103\n",
      "Epoch 2430, Loss: 9.2103\n",
      "Epoch 2435, Loss: 9.2103\n",
      "Epoch 2440, Loss: 9.2103\n",
      "Epoch 2445, Loss: 9.2103\n",
      "Epoch 2450, Loss: 9.2103\n",
      "Epoch 2455, Loss: 9.2103\n",
      "Epoch 2460, Loss: 9.2103\n",
      "Epoch 2465, Loss: 9.2103\n",
      "Epoch 2470, Loss: 9.2103\n",
      "Epoch 2475, Loss: 9.2103\n",
      "Epoch 2480, Loss: 9.2103\n",
      "Epoch 2485, Loss: 9.2103\n",
      "Epoch 2490, Loss: 9.2103\n",
      "Epoch 2495, Loss: 9.2103\n",
      "Epoch 2500, Loss: 9.2103\n",
      "Epoch 2505, Loss: 9.2103\n",
      "Epoch 2510, Loss: 9.2103\n",
      "Epoch 2515, Loss: 9.2103\n",
      "Epoch 2520, Loss: 9.2103\n",
      "Epoch 2525, Loss: 9.2103\n",
      "Epoch 2530, Loss: 9.2103\n",
      "Epoch 2535, Loss: 9.2103\n",
      "Epoch 2540, Loss: 9.2103\n",
      "Epoch 2545, Loss: 9.2103\n",
      "Epoch 2550, Loss: 9.2103\n",
      "Epoch 2555, Loss: 9.2103\n",
      "Epoch 2560, Loss: 9.2103\n",
      "Epoch 2565, Loss: 9.2103\n",
      "Epoch 2570, Loss: 9.2103\n",
      "Epoch 2575, Loss: 9.2103\n",
      "Epoch 2580, Loss: 9.2103\n",
      "Epoch 2585, Loss: 9.2103\n",
      "Epoch 2590, Loss: 9.2103\n",
      "Epoch 2595, Loss: 9.2103\n",
      "Epoch 2600, Loss: 9.2103\n",
      "Epoch 2605, Loss: 9.2103\n",
      "Epoch 2610, Loss: 9.2103\n",
      "Epoch 2615, Loss: 9.2103\n",
      "Epoch 2620, Loss: 9.2103\n",
      "Epoch 2625, Loss: 9.2103\n",
      "Epoch 2630, Loss: 9.2103\n",
      "Epoch 2635, Loss: 9.2103\n",
      "Epoch 2640, Loss: 9.2103\n",
      "Epoch 2645, Loss: 9.2103\n",
      "Epoch 2650, Loss: 9.2103\n",
      "Epoch 2655, Loss: 9.2103\n",
      "Epoch 2660, Loss: 9.2103\n",
      "Epoch 2665, Loss: 9.2103\n",
      "Epoch 2670, Loss: 9.2103\n",
      "Epoch 2675, Loss: 9.2103\n",
      "Epoch 2680, Loss: 9.2103\n",
      "Epoch 2685, Loss: 9.2103\n",
      "Epoch 2690, Loss: 9.2103\n",
      "Epoch 2695, Loss: 9.2103\n",
      "Epoch 2700, Loss: 9.2103\n",
      "Epoch 2705, Loss: 9.2103\n",
      "Epoch 2710, Loss: 9.2103\n",
      "Epoch 2715, Loss: 9.2103\n",
      "Epoch 2720, Loss: 9.2103\n",
      "Epoch 2725, Loss: 9.2103\n",
      "Epoch 2730, Loss: 9.2103\n",
      "Epoch 2735, Loss: 9.2103\n",
      "Epoch 2740, Loss: 9.2103\n",
      "Epoch 2745, Loss: 9.2103\n",
      "Epoch 2750, Loss: 9.2103\n",
      "Epoch 2755, Loss: 9.2103\n",
      "Epoch 2760, Loss: 9.2103\n",
      "Epoch 2765, Loss: 9.2103\n",
      "Epoch 2770, Loss: 9.2103\n",
      "Epoch 2775, Loss: 9.2103\n",
      "Epoch 2780, Loss: 9.2103\n",
      "Epoch 2785, Loss: 9.2103\n",
      "Epoch 2790, Loss: 9.2103\n",
      "Epoch 2795, Loss: 9.2103\n",
      "Epoch 2800, Loss: 9.2103\n",
      "Epoch 2805, Loss: 9.2103\n",
      "Epoch 2810, Loss: 9.2103\n",
      "Epoch 2815, Loss: 9.2103\n",
      "Epoch 2820, Loss: 9.2103\n",
      "Epoch 2825, Loss: 9.2103\n",
      "Epoch 2830, Loss: 9.2103\n",
      "Epoch 2835, Loss: 9.2103\n",
      "Epoch 2840, Loss: 9.2103\n",
      "Epoch 2845, Loss: 9.2103\n",
      "Epoch 2850, Loss: 9.2103\n",
      "Epoch 2855, Loss: 9.2103\n",
      "Epoch 2860, Loss: 9.2103\n",
      "Epoch 2865, Loss: 9.2103\n",
      "Epoch 2870, Loss: 9.2103\n",
      "Epoch 2875, Loss: 9.2103\n",
      "Epoch 2880, Loss: 9.2103\n",
      "Epoch 2885, Loss: 9.2103\n",
      "Epoch 2890, Loss: 9.2103\n",
      "Epoch 2895, Loss: 9.2103\n",
      "Epoch 2900, Loss: 9.2103\n",
      "Epoch 2905, Loss: 9.2103\n",
      "Epoch 2910, Loss: 9.2103\n",
      "Epoch 2915, Loss: 9.2103\n",
      "Epoch 2920, Loss: 9.2103\n",
      "Epoch 2925, Loss: 9.2103\n",
      "Epoch 2930, Loss: 9.2103\n",
      "Epoch 2935, Loss: 9.2103\n",
      "Epoch 2940, Loss: 9.2103\n",
      "Epoch 2945, Loss: 9.2103\n",
      "Epoch 2950, Loss: 9.2103\n",
      "Epoch 2955, Loss: 9.2103\n",
      "Epoch 2960, Loss: 9.2103\n",
      "Epoch 2965, Loss: 9.2103\n",
      "Epoch 2970, Loss: 9.2103\n",
      "Epoch 2975, Loss: 9.2103\n",
      "Epoch 2980, Loss: 9.2103\n",
      "Epoch 2985, Loss: 9.2103\n",
      "Epoch 2990, Loss: 9.2103\n",
      "Epoch 2995, Loss: 9.2103\n",
      "Epoch 3000, Loss: 9.2103\n",
      "Epoch 3005, Loss: 9.2103\n",
      "Epoch 3010, Loss: 9.2103\n",
      "Epoch 3015, Loss: 9.2103\n",
      "Epoch 3020, Loss: 9.2103\n",
      "Epoch 3025, Loss: 9.2103\n",
      "Epoch 3030, Loss: 9.2103\n",
      "Epoch 3035, Loss: 9.2103\n",
      "Epoch 3040, Loss: 9.2103\n",
      "Epoch 3045, Loss: 9.2103\n",
      "Epoch 3050, Loss: 9.2103\n",
      "Epoch 3055, Loss: 9.2103\n",
      "Epoch 3060, Loss: 9.2103\n",
      "Epoch 3065, Loss: 9.2103\n",
      "Epoch 3070, Loss: 9.2103\n",
      "Epoch 3075, Loss: 9.2103\n",
      "Epoch 3080, Loss: 9.2103\n",
      "Epoch 3085, Loss: 9.2103\n",
      "Epoch 3090, Loss: 9.2103\n",
      "Epoch 3095, Loss: 9.2103\n",
      "Epoch 3100, Loss: 9.2103\n",
      "Epoch 3105, Loss: 9.2103\n",
      "Epoch 3110, Loss: 9.2103\n",
      "Epoch 3115, Loss: 9.2103\n",
      "Epoch 3120, Loss: 9.2103\n",
      "Epoch 3125, Loss: 9.2103\n",
      "Epoch 3130, Loss: 9.2103\n",
      "Epoch 3135, Loss: 9.2103\n",
      "Epoch 3140, Loss: 9.2103\n",
      "Epoch 3145, Loss: 9.2103\n",
      "Epoch 3150, Loss: 9.2103\n",
      "Epoch 3155, Loss: 9.2103\n",
      "Epoch 3160, Loss: 9.2103\n",
      "Epoch 3165, Loss: 9.2103\n",
      "Epoch 3170, Loss: 9.2103\n",
      "Epoch 3175, Loss: 9.2103\n",
      "Epoch 3180, Loss: 9.2103\n",
      "Epoch 3185, Loss: 9.2103\n",
      "Epoch 3190, Loss: 9.2103\n",
      "Epoch 3195, Loss: 9.2103\n",
      "Epoch 3200, Loss: 9.2103\n",
      "Epoch 3205, Loss: 9.2103\n",
      "Epoch 3210, Loss: 9.2103\n",
      "Epoch 3215, Loss: 9.2103\n",
      "Epoch 3220, Loss: 9.2103\n",
      "Epoch 3225, Loss: 9.2103\n",
      "Epoch 3230, Loss: 9.2103\n",
      "Epoch 3235, Loss: 9.2103\n",
      "Epoch 3240, Loss: 9.2103\n",
      "Epoch 3245, Loss: 9.2103\n",
      "Epoch 3250, Loss: 9.2103\n",
      "Epoch 3255, Loss: 9.2103\n",
      "Epoch 3260, Loss: 9.2103\n",
      "Epoch 3265, Loss: 9.2103\n",
      "Epoch 3270, Loss: 9.2103\n",
      "Epoch 3275, Loss: 9.2103\n",
      "Epoch 3280, Loss: 9.2103\n",
      "Epoch 3285, Loss: 9.2103\n",
      "Epoch 3290, Loss: 9.2103\n",
      "Epoch 3295, Loss: 9.2103\n",
      "Epoch 3300, Loss: 9.2103\n",
      "Epoch 3305, Loss: 9.2103\n",
      "Epoch 3310, Loss: 9.2103\n",
      "Epoch 3315, Loss: 9.2103\n",
      "Epoch 3320, Loss: 9.2103\n",
      "Epoch 3325, Loss: 9.2103\n",
      "Epoch 3330, Loss: 9.2103\n",
      "Epoch 3335, Loss: 9.2103\n",
      "Epoch 3340, Loss: 9.2103\n",
      "Epoch 3345, Loss: 9.2103\n",
      "Epoch 3350, Loss: 9.2103\n",
      "Epoch 3355, Loss: 9.2103\n",
      "Epoch 3360, Loss: 9.2103\n",
      "Epoch 3365, Loss: 9.2103\n",
      "Epoch 3370, Loss: 9.2103\n",
      "Epoch 3375, Loss: 9.2103\n",
      "Epoch 3380, Loss: 9.2103\n",
      "Epoch 3385, Loss: 9.2103\n",
      "Epoch 3390, Loss: 9.2103\n",
      "Epoch 3395, Loss: 9.2103\n",
      "Epoch 3400, Loss: 9.2103\n",
      "Epoch 3405, Loss: 9.2103\n",
      "Epoch 3410, Loss: 9.2103\n",
      "Epoch 3415, Loss: 9.2103\n",
      "Epoch 3420, Loss: 9.2103\n",
      "Epoch 3425, Loss: 9.2103\n",
      "Epoch 3430, Loss: 9.2103\n",
      "Epoch 3435, Loss: 9.2103\n",
      "Epoch 3440, Loss: 9.2103\n",
      "Epoch 3445, Loss: 9.2103\n",
      "Epoch 3450, Loss: 9.2103\n",
      "Epoch 3455, Loss: 9.2103\n",
      "Epoch 3460, Loss: 9.2103\n",
      "Epoch 3465, Loss: 9.2103\n",
      "Epoch 3470, Loss: 9.2103\n",
      "Epoch 3475, Loss: 9.2103\n",
      "Epoch 3480, Loss: 9.2103\n",
      "Epoch 3485, Loss: 9.2103\n",
      "Epoch 3490, Loss: 9.2103\n",
      "Epoch 3495, Loss: 9.2103\n",
      "Epoch 3500, Loss: 9.2103\n",
      "Epoch 3505, Loss: 9.2103\n",
      "Epoch 3510, Loss: 9.2103\n",
      "Epoch 3515, Loss: 9.2103\n",
      "Epoch 3520, Loss: 9.2103\n",
      "Epoch 3525, Loss: 9.2103\n",
      "Epoch 3530, Loss: 9.2103\n",
      "Epoch 3535, Loss: 9.2103\n",
      "Epoch 3540, Loss: 9.2103\n",
      "Epoch 3545, Loss: 9.2103\n",
      "Epoch 3550, Loss: 9.2103\n",
      "Epoch 3555, Loss: 9.2103\n",
      "Epoch 3560, Loss: 9.2103\n",
      "Epoch 3565, Loss: 9.2103\n",
      "Epoch 3570, Loss: 9.2103\n",
      "Epoch 3575, Loss: 9.2103\n",
      "Epoch 3580, Loss: 9.2103\n",
      "Epoch 3585, Loss: 9.2103\n",
      "Epoch 3590, Loss: 9.2103\n",
      "Epoch 3595, Loss: 9.2103\n",
      "Epoch 3600, Loss: 9.2103\n",
      "Epoch 3605, Loss: 9.2103\n",
      "Epoch 3610, Loss: 9.2103\n",
      "Epoch 3615, Loss: 9.2103\n",
      "Epoch 3620, Loss: 9.2103\n",
      "Epoch 3625, Loss: 9.2103\n",
      "Epoch 3630, Loss: 9.2103\n",
      "Epoch 3635, Loss: 9.2103\n",
      "Epoch 3640, Loss: 9.2103\n",
      "Epoch 3645, Loss: 9.2103\n",
      "Epoch 3650, Loss: 9.2103\n",
      "Epoch 3655, Loss: 9.2103\n",
      "Epoch 3660, Loss: 9.2103\n",
      "Epoch 3665, Loss: 9.2103\n",
      "Epoch 3670, Loss: 9.2103\n",
      "Epoch 3675, Loss: 9.2103\n",
      "Epoch 3680, Loss: 9.2103\n",
      "Epoch 3685, Loss: 9.2103\n",
      "Epoch 3690, Loss: 9.2103\n",
      "Epoch 3695, Loss: 9.2103\n",
      "Epoch 3700, Loss: 9.2103\n",
      "Epoch 3705, Loss: 9.2103\n",
      "Epoch 3710, Loss: 9.2103\n",
      "Epoch 3715, Loss: 9.2103\n",
      "Epoch 3720, Loss: 9.2103\n",
      "Epoch 3725, Loss: 9.2103\n",
      "Epoch 3730, Loss: 9.2103\n",
      "Epoch 3735, Loss: 9.2103\n",
      "Epoch 3740, Loss: 9.2103\n",
      "Epoch 3745, Loss: 9.2103\n",
      "Epoch 3750, Loss: 9.2103\n",
      "Epoch 3755, Loss: 9.2103\n",
      "Epoch 3760, Loss: 9.2103\n",
      "Epoch 3765, Loss: 9.2103\n",
      "Epoch 3770, Loss: 9.2103\n",
      "Epoch 3775, Loss: 9.2103\n",
      "Epoch 3780, Loss: 9.2103\n",
      "Epoch 3785, Loss: 9.2103\n",
      "Epoch 3790, Loss: 9.2103\n",
      "Epoch 3795, Loss: 9.2103\n",
      "Epoch 3800, Loss: 9.2103\n",
      "Epoch 3805, Loss: 9.2103\n",
      "Epoch 3810, Loss: 9.2103\n",
      "Epoch 3815, Loss: 9.2103\n",
      "Epoch 3820, Loss: 9.2103\n",
      "Epoch 3825, Loss: 9.2103\n",
      "Epoch 3830, Loss: 9.2103\n",
      "Epoch 3835, Loss: 9.2103\n",
      "Epoch 3840, Loss: 9.2103\n",
      "Epoch 3845, Loss: 9.2103\n",
      "Epoch 3850, Loss: 9.2103\n",
      "Epoch 3855, Loss: 9.2103\n",
      "Epoch 3860, Loss: 9.2103\n",
      "Epoch 3865, Loss: 9.2103\n",
      "Epoch 3870, Loss: 9.2103\n",
      "Epoch 3875, Loss: 9.2103\n",
      "Epoch 3880, Loss: 9.2103\n",
      "Epoch 3885, Loss: 9.2103\n",
      "Epoch 3890, Loss: 9.2103\n",
      "Epoch 3895, Loss: 9.2103\n",
      "Epoch 3900, Loss: 9.2103\n",
      "Epoch 3905, Loss: 9.2103\n",
      "Epoch 3910, Loss: 9.2103\n",
      "Epoch 3915, Loss: 9.2103\n",
      "Epoch 3920, Loss: 9.2103\n",
      "Epoch 3925, Loss: 9.2103\n",
      "Epoch 3930, Loss: 9.2103\n",
      "Epoch 3935, Loss: 9.2103\n",
      "Epoch 3940, Loss: 9.2103\n",
      "Epoch 3945, Loss: 9.2103\n",
      "Epoch 3950, Loss: 9.2103\n",
      "Epoch 3955, Loss: 9.2103\n",
      "Epoch 3960, Loss: 9.2103\n",
      "Epoch 3965, Loss: 9.2103\n",
      "Epoch 3970, Loss: 9.2103\n",
      "Epoch 3975, Loss: 9.2103\n",
      "Epoch 3980, Loss: 9.2103\n",
      "Epoch 3985, Loss: 9.2103\n",
      "Epoch 3990, Loss: 9.2103\n",
      "Epoch 3995, Loss: 9.2103\n",
      "Epoch 4000, Loss: 9.2103\n",
      "Epoch 4005, Loss: 9.2103\n",
      "Epoch 4010, Loss: 9.2103\n",
      "Epoch 4015, Loss: 9.2103\n",
      "Epoch 4020, Loss: 9.2103\n",
      "Epoch 4025, Loss: 9.2103\n",
      "Epoch 4030, Loss: 9.2103\n",
      "Epoch 4035, Loss: 9.2103\n",
      "Epoch 4040, Loss: 9.2103\n",
      "Epoch 4045, Loss: 9.2103\n",
      "Epoch 4050, Loss: 9.2103\n",
      "Epoch 4055, Loss: 9.2103\n",
      "Epoch 4060, Loss: 9.2103\n",
      "Epoch 4065, Loss: 9.2103\n",
      "Epoch 4070, Loss: 9.2103\n",
      "Epoch 4075, Loss: 9.2103\n",
      "Epoch 4080, Loss: 9.2103\n",
      "Epoch 4085, Loss: 9.2103\n",
      "Epoch 4090, Loss: 9.2103\n",
      "Epoch 4095, Loss: 9.2103\n",
      "Epoch 4100, Loss: 9.2103\n",
      "Epoch 4105, Loss: 9.2103\n",
      "Epoch 4110, Loss: 9.2103\n",
      "Epoch 4115, Loss: 9.2103\n",
      "Epoch 4120, Loss: 9.2103\n",
      "Epoch 4125, Loss: 9.2103\n",
      "Epoch 4130, Loss: 9.2103\n",
      "Epoch 4135, Loss: 9.2103\n",
      "Epoch 4140, Loss: 9.2103\n",
      "Epoch 4145, Loss: 9.2103\n",
      "Epoch 4150, Loss: 9.2103\n",
      "Epoch 4155, Loss: 9.2103\n",
      "Epoch 4160, Loss: 9.2103\n",
      "Epoch 4165, Loss: 9.2103\n",
      "Epoch 4170, Loss: 9.2103\n",
      "Epoch 4175, Loss: 9.2103\n",
      "Epoch 4180, Loss: 9.2103\n",
      "Epoch 4185, Loss: 9.2103\n",
      "Epoch 4190, Loss: 9.2103\n",
      "Epoch 4195, Loss: 9.2103\n",
      "Epoch 4200, Loss: 9.2103\n",
      "Epoch 4205, Loss: 9.2103\n",
      "Epoch 4210, Loss: 9.2103\n",
      "Epoch 4215, Loss: 9.2103\n",
      "Epoch 4220, Loss: 9.2103\n",
      "Epoch 4225, Loss: 9.2103\n",
      "Epoch 4230, Loss: 9.2103\n",
      "Epoch 4235, Loss: 9.2103\n",
      "Epoch 4240, Loss: 9.2103\n",
      "Epoch 4245, Loss: 9.2103\n",
      "Epoch 4250, Loss: 9.2103\n",
      "Epoch 4255, Loss: 9.2103\n",
      "Epoch 4260, Loss: 9.2103\n",
      "Epoch 4265, Loss: 9.2103\n",
      "Epoch 4270, Loss: 9.2103\n",
      "Epoch 4275, Loss: 9.2103\n",
      "Epoch 4280, Loss: 9.2103\n",
      "Epoch 4285, Loss: 9.2103\n",
      "Epoch 4290, Loss: 9.2103\n",
      "Epoch 4295, Loss: 9.2103\n",
      "Epoch 4300, Loss: 9.2103\n",
      "Epoch 4305, Loss: 9.2103\n",
      "Epoch 4310, Loss: 9.2103\n",
      "Epoch 4315, Loss: 9.2103\n",
      "Epoch 4320, Loss: 9.2103\n",
      "Epoch 4325, Loss: 9.2103\n",
      "Epoch 4330, Loss: 9.2103\n",
      "Epoch 4335, Loss: 9.2103\n",
      "Epoch 4340, Loss: 9.2103\n",
      "Epoch 4345, Loss: 9.2103\n",
      "Epoch 4350, Loss: 9.2103\n",
      "Epoch 4355, Loss: 9.2103\n",
      "Epoch 4360, Loss: 9.2103\n",
      "Epoch 4365, Loss: 9.2103\n",
      "Epoch 4370, Loss: 9.2103\n",
      "Epoch 4375, Loss: 9.2103\n",
      "Epoch 4380, Loss: 9.2103\n",
      "Epoch 4385, Loss: 9.2103\n",
      "Epoch 4390, Loss: 9.2103\n",
      "Epoch 4395, Loss: 9.2103\n",
      "Epoch 4400, Loss: 9.2103\n",
      "Epoch 4405, Loss: 9.2103\n",
      "Epoch 4410, Loss: 9.2103\n",
      "Epoch 4415, Loss: 9.2103\n",
      "Epoch 4420, Loss: 9.2103\n",
      "Epoch 4425, Loss: 9.2103\n",
      "Epoch 4430, Loss: 9.2103\n",
      "Epoch 4435, Loss: 9.2103\n",
      "Epoch 4440, Loss: 9.2103\n",
      "Epoch 4445, Loss: 9.2103\n",
      "Epoch 4450, Loss: 9.2103\n",
      "Epoch 4455, Loss: 9.2103\n",
      "Epoch 4460, Loss: 9.2103\n",
      "Epoch 4465, Loss: 9.2103\n",
      "Epoch 4470, Loss: 9.2103\n",
      "Epoch 4475, Loss: 9.2103\n",
      "Epoch 4480, Loss: 9.2103\n",
      "Epoch 4485, Loss: 9.2103\n",
      "Epoch 4490, Loss: 9.2103\n",
      "Epoch 4495, Loss: 9.2103\n",
      "Epoch 4500, Loss: 9.2103\n",
      "Epoch 4505, Loss: 9.2103\n",
      "Epoch 4510, Loss: 9.2103\n",
      "Epoch 4515, Loss: 9.2103\n",
      "Epoch 4520, Loss: 9.2103\n",
      "Epoch 4525, Loss: 9.2103\n",
      "Epoch 4530, Loss: 9.2103\n",
      "Epoch 4535, Loss: 9.2103\n",
      "Epoch 4540, Loss: 9.2103\n",
      "Epoch 4545, Loss: 9.2103\n",
      "Epoch 4550, Loss: 9.2103\n",
      "Epoch 4555, Loss: 9.2103\n",
      "Epoch 4560, Loss: 9.2103\n",
      "Epoch 4565, Loss: 9.2103\n",
      "Epoch 4570, Loss: 9.2103\n",
      "Epoch 4575, Loss: 9.2103\n",
      "Epoch 4580, Loss: 9.2103\n",
      "Epoch 4585, Loss: 9.2103\n",
      "Epoch 4590, Loss: 9.2103\n",
      "Epoch 4595, Loss: 9.2103\n",
      "Epoch 4600, Loss: 9.2103\n",
      "Epoch 4605, Loss: 9.2103\n",
      "Epoch 4610, Loss: 9.2103\n",
      "Epoch 4615, Loss: 9.2103\n",
      "Epoch 4620, Loss: 9.2103\n",
      "Epoch 4625, Loss: 9.2103\n",
      "Epoch 4630, Loss: 9.2103\n",
      "Epoch 4635, Loss: 9.2103\n",
      "Epoch 4640, Loss: 9.2103\n",
      "Epoch 4645, Loss: 9.2103\n",
      "Epoch 4650, Loss: 9.2103\n",
      "Epoch 4655, Loss: 9.2103\n",
      "Epoch 4660, Loss: 9.2103\n",
      "Epoch 4665, Loss: 9.2103\n",
      "Epoch 4670, Loss: 9.2103\n",
      "Epoch 4675, Loss: 9.2103\n",
      "Epoch 4680, Loss: 9.2103\n",
      "Epoch 4685, Loss: 9.2103\n",
      "Epoch 4690, Loss: 9.2103\n",
      "Epoch 4695, Loss: 9.2103\n",
      "Epoch 4700, Loss: 9.2103\n",
      "Epoch 4705, Loss: 9.2103\n",
      "Epoch 4710, Loss: 9.2103\n",
      "Epoch 4715, Loss: 9.2103\n",
      "Epoch 4720, Loss: 9.2103\n",
      "Epoch 4725, Loss: 9.2103\n",
      "Epoch 4730, Loss: 9.2103\n",
      "Epoch 4735, Loss: 9.2103\n",
      "Epoch 4740, Loss: 9.2103\n",
      "Epoch 4745, Loss: 9.2103\n",
      "Epoch 4750, Loss: 9.2103\n",
      "Epoch 4755, Loss: 9.2103\n",
      "Epoch 4760, Loss: 9.2103\n",
      "Epoch 4765, Loss: 9.2103\n",
      "Epoch 4770, Loss: 9.2103\n",
      "Epoch 4775, Loss: 9.2103\n",
      "Epoch 4780, Loss: 9.2103\n",
      "Epoch 4785, Loss: 9.2103\n",
      "Epoch 4790, Loss: 9.2103\n",
      "Epoch 4795, Loss: 9.2103\n",
      "Epoch 4800, Loss: 9.2103\n",
      "Epoch 4805, Loss: 9.2103\n",
      "Epoch 4810, Loss: 9.2103\n",
      "Epoch 4815, Loss: 9.2103\n",
      "Epoch 4820, Loss: 9.2103\n",
      "Epoch 4825, Loss: 9.2103\n",
      "Epoch 4830, Loss: 9.2103\n",
      "Epoch 4835, Loss: 9.2103\n",
      "Epoch 4840, Loss: 9.2103\n",
      "Epoch 4845, Loss: 9.2103\n",
      "Epoch 4850, Loss: 9.2103\n",
      "Epoch 4855, Loss: 9.2103\n",
      "Epoch 4860, Loss: 9.2103\n",
      "Epoch 4865, Loss: 9.2103\n",
      "Epoch 4870, Loss: 9.2103\n",
      "Epoch 4875, Loss: 9.2103\n",
      "Epoch 4880, Loss: 9.2103\n",
      "Epoch 4885, Loss: 9.2103\n",
      "Epoch 4890, Loss: 9.2103\n",
      "Epoch 4895, Loss: 9.2103\n",
      "Epoch 4900, Loss: 9.2103\n",
      "Epoch 4905, Loss: 9.2103\n",
      "Epoch 4910, Loss: 9.2103\n",
      "Epoch 4915, Loss: 9.2103\n",
      "Epoch 4920, Loss: 9.2103\n",
      "Epoch 4925, Loss: 9.2103\n",
      "Epoch 4930, Loss: 9.2103\n",
      "Epoch 4935, Loss: 9.2103\n",
      "Epoch 4940, Loss: 9.2103\n",
      "Epoch 4945, Loss: 9.2103\n",
      "Epoch 4950, Loss: 9.2103\n",
      "Epoch 4955, Loss: 9.2103\n",
      "Epoch 4960, Loss: 9.2103\n",
      "Epoch 4965, Loss: 9.2103\n",
      "Epoch 4970, Loss: 9.2103\n",
      "Epoch 4975, Loss: 9.2103\n",
      "Epoch 4980, Loss: 9.2103\n",
      "Epoch 4985, Loss: 9.2103\n",
      "Epoch 4990, Loss: 9.2103\n",
      "Epoch 4995, Loss: 9.2103\n",
      "Epoch 5000, Loss: 9.2103\n"
     ]
    }
   ],
   "source": [
    "# There is a big training loop but you can view different parts seprated at the end of the notebook\n",
    "\n",
    "num_epochs = 5000\n",
    "\n",
    "for epoch in range(1, num_epochs+1):\n",
    "    t += 1\n",
    "    a = X\n",
    "    after_act = {}\n",
    "    # Forwardpass\n",
    "    for index, (key, value) in enumerate(network.items()):\n",
    "        if key == 'Input':\n",
    "            continue\n",
    "        elif 'Hidden' in key:\n",
    "            w = weights[f'Hidden_{index}']\n",
    "            b = biases[f'Hidden_{index}']\n",
    "            a = feedforward(w, b, a, activations['relu'])\n",
    "            after_act[f'Hidden_{index}'] = a\n",
    "        else:\n",
    "            w = weights['Output']\n",
    "            b = biases['Output']\n",
    "            a = feedforward(w, b, a, activations['sigmoid'])\n",
    "            after_act['Output'] = a\n",
    "        \n",
    "    # Computing delta\n",
    "    deltas = {}\n",
    "    error = y - after_act['Output']\n",
    "    deltas['Output'] = error * sigmoid_derivative(after_act['Output'])\n",
    "    \n",
    "    hidden_layers = [k for k in weights.keys() if 'Hidden' in k]\n",
    "    hidden_layers.sort(reverse=True)\n",
    "    next_layer = 'Output'\n",
    "    for layer in hidden_layers:\n",
    "        deltas[layer] = np.dot(deltas[next_layer], weights[next_layer].T) * relu_derivative(after_act[layer])\n",
    "        next_layer = layer\n",
    "\n",
    "        \n",
    "    # Gradients\n",
    "    # I know I coded this part really weird-_-\n",
    "    grad_b = {key: np.sum(delta, axis=0, keepdims=True) for key, delta in deltas.items()}\n",
    "    grad_b = dict(reversed(list(grad_b.items())))\n",
    "\n",
    "    grad_w = {}\n",
    "    for key in deltas:\n",
    "        prev_layer = X if 'Hidden_1' in key else after_act[list(after_act.keys())[list(deltas.keys()).index(key)-1]]\n",
    "        grad_w[key] = np.dot(prev_layer.T, deltas[key])\n",
    "    grad_w = dict(reversed(list(grad_w.items())))\n",
    "\n",
    "    # Optimization\n",
    "    for key in weights:\n",
    "        g_w = grad_w[key]\n",
    "        g_b = grad_b[key]\n",
    "        \n",
    "        # First moment\n",
    "        m_w[key] = beta1 * m_w[key] + (1 - beta1) * g_w\n",
    "        m_b[key] = beta1 * m_b[key] + (1 - beta1) * g_b\n",
    "        \n",
    "        # Second moment\n",
    "        v_w[key] = beta2 * v_w[key] + (1 - beta2) * (g_w ** 2)\n",
    "        v_b[key] = beta2 * v_b[key] + (1 - beta2) * (g_b ** 2)\n",
    "        \n",
    "        # Bias-corrected\n",
    "        m_hat_w = m_w[key] / (1 - beta1**t)\n",
    "        v_hat_w = v_w[key] / (1 - beta2**t)\n",
    "        m_hat_b = m_b[key] / (1 - beta1**t)\n",
    "        v_hat_b = v_b[key] / (1 - beta2**t)\n",
    "        \n",
    "        # Update\n",
    "        weights[key] -= lr * m_hat_w / (np.sqrt(v_hat_w) + epsilon)\n",
    "        biases[key]  -= lr * m_hat_b / (np.sqrt(v_hat_b) + epsilon)\n",
    "\n",
    "    # loss Function\n",
    "    y_hat = np.clip(after_act['Output'], epsilon, 1 - epsilon)\n",
    "    loss = -np.mean(y * np.log(y_hat) + (1 - y) * np.log(1 - y_hat))\n",
    "    if epoch % 5 == 0:\n",
    "        print(f\"Epoch {epoch}, Loss: {loss:.4f}\")   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "236a97ac-84d4-4dba-8111-cdb4a9980319",
   "metadata": {},
   "outputs": [],
   "source": [
    "    for index, (key, value) in enumerate(network.items()):\n",
    "        if key == 'Input':\n",
    "            pass\n",
    "        elif 'Hidden' in key:\n",
    "            w = init_weights(list(network.values())[index-1], list(network.values())[index],activations['relu'])\n",
    "            b = init_bias(value)\n",
    "            \n",
    "            assert w.shape[0] == a.shape[1] # Preventing error because of a shape mismatch between matrices\n",
    "            a = feedforward(w,b,a, activations[\"relu\"])\n",
    "            \n",
    "            weights[f\"Hidden_{index}\"] = w\n",
    "            biases[f\"Hidden_{index}\"] = b\n",
    "            after_act[f\"Hidden_{index}\"] = a\n",
    "            \n",
    "        else:\n",
    "            w = init_weights(list(network.values())[index-1], list(network.values())[index],activations['sigmoid']) # dicts don't have index function, converting to list first\n",
    "            b = init_bias(value)\n",
    "            \n",
    "            assert w.shape[0] == a.shape[1] # Preventing error because of a shape mismatch between matrices\n",
    "            a = feedforward(w,b,a, activations[\"sigmoid\"])\n",
    "            \n",
    "            weights[f\"Output\"] = w\n",
    "            biases[f\"Output\"] = b\n",
    "            after_act[f\"Output\"] = a\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0debc97-91bf-4013-9e31-27e8db9c4e55",
   "metadata": {},
   "source": [
    "#### FeedForward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cd894ea-edb8-4991-963d-2d9cea394a16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def feedforward(W:np.ndarray, b:np.ndarray, X:np.ndarray, activation_func:callable):\n",
    "#     z = (np.dot(X,W)) + b\n",
    "#     a = activation_func(z) # adding activation function \n",
    "#     return a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dae67a9f-169e-4e79-89c9-1503a88d8a4e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# a = X\n",
    "# weights = {} # storing different layers weights inside a dict.\n",
    "# biases = {}  # same for biases.\n",
    "# after_act = {}\n",
    "\n",
    "# # A loop  for initializing parameters and feedforward\n",
    "# for index, (key, value) in enumerate(network.items()):\n",
    "#     if key == 'Input':\n",
    "#         pass\n",
    "#     elif 'Hidden' in key:\n",
    "#         w = init_weights(list(network.values())[index-1], list(network.values())[index],activations['relu'])\n",
    "#         b = init_bias(value)\n",
    "        \n",
    "#         assert w.shape[0] == a.shape[1] # Preventing error because of a shape mismatch between matrices\n",
    "#         a = feedforward(w,b,a, activations[\"relu\"])\n",
    "        \n",
    "#         weights[f\"Hidden_{index}\"] = w\n",
    "#         biases[f\"Hidden_{index}\"] = b\n",
    "#         after_act[f\"Hidden_{index}\"] = a\n",
    "        \n",
    "#     else:\n",
    "#         w = init_weights(list(network.values())[index-1], list(network.values())[index],activations['sigmoid']) # dicts don't have index function, converting to list first\n",
    "#         b = init_bias(value)\n",
    "        \n",
    "#         assert w.shape[0] == a.shape[1] # Preventing error because of a shape mismatch between matrices\n",
    "#         a = feedforward(w,b,a, activations[\"sigmoid\"])\n",
    "        \n",
    "#         weights[f\"Output\"] = w\n",
    "#         biases[f\"Output\"] = b\n",
    "#         after_act[f\"Output\"] = a\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40cc1280-c975-4f1f-a181-5b36faa7e078",
   "metadata": {},
   "outputs": [],
   "source": [
    "# epsilon = 1e-15 # log(0) is undefined\n",
    "# y_hat = np.clip(after_act['Output'], epsilon, 1 - epsilon)\n",
    "# loss = -np.mean(y * np.log(y_hat) + (1 - y) * np.log(1 - y_hat))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4ce65bd-24bd-46ca-b271-fe9e310af75f",
   "metadata": {},
   "source": [
    "#### BackPropagation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8635b4fd-cdaa-4f53-bea9-af89f77278c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Calculating the error and delta\n",
    "\n",
    "# deltas = {}\n",
    "\n",
    "# # Output layer delta\n",
    "# error = y - after_act['Output']\n",
    "# deltas['Output'] = error * sigmoid_derivative(after_act['Output'])\n",
    "\n",
    "# # Hidden layers in reverse order\n",
    "# hidden_layers = [k for k in weights.keys() if 'Hidden' in k]\n",
    "# hidden_layers.sort(reverse=True)  # or reverse order of insertion if needed\n",
    "\n",
    "# next_layer = 'Output'\n",
    "# for layer in hidden_layers:\n",
    "#     deltas[layer] = np.dot(deltas[next_layer], weights[next_layer].T) * relu_derivative(after_act[layer])\n",
    "#     next_layer = layer\n",
    "\n",
    "# # Calcuating gradient\n",
    "# # bias\n",
    "# grad_b = np.sum(deltas, axis=0, keepdims=True)\n",
    "# grad_b = dict(reversed(list(grad_b.items())))\n",
    "\n",
    "# # weight\n",
    "# grad_w = {}\n",
    "# # I know I coded this part really weird-_-\n",
    "# for index_d, (key_d, value_d) in enumerate(deltas.items()):\n",
    "#     for index_a, (key_a, value_a) in enumerate(after_act.items()):\n",
    "#         if key_d == key_a:\n",
    "#             if key_d == 'Hidden_1':\n",
    "#                 grad_w[key_d] = np.dot(X.T, value_d)\n",
    "#             else:\n",
    "#                 grad_w[key_d] = np.dot(np.transpose(list(after_act.values())[index_a-1]), value_d)\n",
    "# # The order was incorrect\n",
    "# grad_w = dict(reversed(list(grad_w.items())))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2018216-456f-41a7-bd32-c8854af6589e",
   "metadata": {},
   "source": [
    "#### Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a32add0c-7af9-4bac-ab35-6a8714514b2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Hyperparameters\n",
    "# lr = 0.01        # learning rate\n",
    "# beta1 = 0.9      # momentum term\n",
    "# beta2 = 0.999    # RMSprop term\n",
    "# epsilon = 1e-8   # small value to prevent division by zero\n",
    "\n",
    "# # Initialize m, v for all weights and biases\n",
    "# m_w = {k: np.zeros_like(v) for k, v in weights.items()}\n",
    "# v_w = {k: np.zeros_like(v) for k, v in weights.items()}\n",
    "# m_b = {k: np.zeros_like(v) for k, v in biases.items()}\n",
    "# v_b = {k: np.zeros_like(v) for k, v in biases.items()}\n",
    "\n",
    "# t = 0  # timestep\n",
    "\n",
    "# # Inside your training loop:\n",
    "# t += 1\n",
    "\n",
    "# for key in weights:\n",
    "#     # Get gradients (from backprop)\n",
    "#     g_w = grad_w[key]\n",
    "#     g_b = grad_b[key]\n",
    "\n",
    "#     # Update biased first moment estimate\n",
    "#     m_w[key] = beta1 * m_w[key] + (1 - beta1) * g_w\n",
    "#     m_b[key] = beta1 * m_b[key] + (1 - beta1) * g_b\n",
    "\n",
    "#     # Update biased second raw moment estimate\n",
    "#     v_w[key] = beta2 * v_w[key] + (1 - beta2) * (g_w ** 2)\n",
    "#     v_b[key] = beta2 * v_b[key] + (1 - beta2) * (g_b ** 2)\n",
    "\n",
    "#     # Compute bias-corrected first and second moment estimates\n",
    "#     m_hat_w = m_w[key] / (1 - beta1 ** t)\n",
    "#     v_hat_w = v_w[key] / (1 - beta2 ** t)\n",
    "#     m_hat_b = m_b[key] / (1 - beta1 ** t)\n",
    "#     v_hat_b = v_b[key] / (1 - beta2 ** t)\n",
    "\n",
    "#     # Update weights and biases\n",
    "#     weights[key] -= lr * m_hat_w / (np.sqrt(v_hat_w) + epsilon)\n",
    "#     biases[key]  -= lr * m_hat_b / (np.sqrt(v_hat_b) + epsilon)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f4a96b0-9bf6-46e7-ad75-f56873156c89",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
